The distributed AE works using three classes:
	OME::Analysis::Engine::SimpleWorkerExecutor
		implements the OME::Analysis::Engine::Executor interface
	OME::Analysis::Engine::Worker
		a DBObject to maintain a list of workers available to the back-end
	OME/Analysis/Engine/NonblockingSlaveWorkerCGI.pl
		a worker implemented as a non-blocking CGI script

To set it up, you will need to do the following:

Setup the Workers
--------------------------------
On EACH worker node (a node can host several workers):

* Install OME (sudo perl install.pl). OMEIS and a local database is optional

* Copy src/perl2/OME/Analysis/Engine/NonblockingSlaveWorkerCGI.pl into the appropriate
Apache accessible directory so that you could specify a URL that would execute
the NonblockingSlaveWorkerCGI.pl as a mod_pelr CGI e.g:

sudo cp src/perl2/OME/Analysis/Engine/NonblockingSlaveWorkerCGI.pl /OME/perl2/
The URL would then be http://worker123.node.com/perl2/NonblockingSlaveWorkerCGI.pl

* Set the maximum workers the node can host by running 
src/perl2/OME/Analysis/Engine/install_worker_node.pl as root.

----       Matlab Notes     ----
* If you would like the worker to run matlab, make sure the matlab libraries 
are included in the dynamic linking path. The recommended way to do this is
given below. Change the matlab library path to match your system

	sudo echo "/usr/local/matlab/bin/glnxa64" > /etc/ld.so.conf.d/matlab.conf
	sudo /sbin/ldconfig

* Also, change the matlab license to the apache user. e.g.
	sed -e's/current_matlab_user/www/g' /usr/local/matlab/etc/MLM.opt > apacheMLM.opt
	sudo cp apacheMLM.opt /usr/local/matlab/etc/MLM.opt
	sudo /usr/local/matlab/etc/lmdown
	/usr/local/matlab/etc/lmstart
	# Change permissions or remove log files as needed to get lmstart working. 
	# It will typically give you error messages and paths.


Additional Setup to Back End to support remote Workers
-------------------------------------------------
* If you will be using remote workers, you will need to configure
pg_hba.conf which lives on the back-end in pgsql/data/ (the postgres 'home') to
allow the remote workers to connect to your DB.  Usually a trusted
authentication from specified IPs is sufficient:

host       all         127.0.0.1     255.255.255.255    trust
host       all         x.x.x.x     255.255.255.255    trust

Replace x.x.x.x with the IP address of your worker. The first line allows 
connections from the local machine in the style of: 
	psql -h [HOSTNAME] -d ome

As distributed analysis gets incorporated into the system more, these steps will gradually
disappear.

Setup the Back End:
-------------------

* Install OME. When prompted for "Host" during "Database Bootstrap" section, 
you must specify the hostname of the machine you are installing on. This 
hostname must be resolvable on your local network.

* Manually populate the analysis_workers table with url and status (set to
'IDLE') for each worker on each node.  A node that hosts more than one worker
needs an entry in this table for every worker it has.  The URL should execute
OME/Analysis/Engine/NonblockingSlaveWorkerCGI.pl as a mod_perl CGI.

e.g, to install a localhost worker after copying NonblockingSlaveWorkerCGI.pl
to the same directory as serve.pl:

INSERT INTO analysis_workers(status, url) 
VALUES ('IDLE','http://localhost/perl2/NonblockingSlaveWorkerCGI.pl');

* And lastly, the OME::Analysis::Engine::SimpleWorkerExecutor is selected as
the executor by setting a variable in the CONFIGURATION table:
UPDATE configuration SET value='OME::Analysis::Engine::SimpleWorkerExecutor'
WHERE name='executor';
The default value is 'OME::Analysis::Engine::UnthreadedPerlExecutor'.
You must redo this step if you ever re-run the installer.

--------------------------
Some implementation notes:
--------------------------
The Analysis Engine calls the executor's executeModule() for each module that
can execute concurrently.  If executeModule() is non-blocking, then the AE will
keep calling it until its finished with a "ROUND".  At this point, it will call
waitForAnyModules() and when this returns, tries to go on to the next round. 
Finally it will call waitForAllModules to let the executor finish.

The SimpleWorkerExecutor uses a FIFO stack to manage jobs.  As the AE calls
executeModule(), a job is added to the queue.  Before executeModule() returns,
a call is made to shiftQueue(), which attempts to shift jobs off the queue
until the queue is empty or no workers are found, or a worker reports an error.
All of these calls are non-blocking.

Workers are used in a round-robin fashion.  shiftQueue gets a worker that's
been idle for the longest time by calling the DB directly to return the first
worker with an 'IDLE' status from a list sorted by the last_used timestamp.  If
the DB returns a worker, its assigned the job and the process repeats until
there are no idle workers or no jobs in the queue, or a worker returns an
error, at which point shiftQueue returns.

The method used to assign a job to a worker is called pressWorker().  If the
worker returns an error response, the worker's last_used is updated to 'now()'
so that it goes to the back of the line.  The worker call is partially
blocking:  The worker blocks until it can establish a remote session, 
gather all of its required parameters without error, and change its own status
on the back-end to 'BUSY'.  The worker then sends a response (200 OK) before 
proceeding to actually execute the module.

The worker's delayed (post-responce) module execution is accomplished by 
registering a cleanup task in the Apache request processing stack.  This is done
through the OME API OME::Fork->doLater() call.  The cleanup task executes the 
module and then sends the DB all the notices  specified in the Notices parameter
using OME::Tasks::NotificationManager->notify().  There are two kinds of Notices
set up by the SimpleWorkerExecutor:  One that includes a unique ID string for 
that executor instance, and one that is generic 
(The notices are 'WorkerIdle_UniqueID' and 'WorkerIdle').  There is also a backup
mechanism for the notices.  While its executing (status() = 'BUSY'), the worker
sets its master field to the MasterID passed in from the caller.  This MasterID
allows an executor instance to identify its own busy workers.

Worker nodes can be used by any back-end, so steps are taken to ensure than no
more than the specified maximum number of workers are ever running on the same
node - regardless of the back-end(s) they are connected to.  This is done by
maintaining worker PIDs in /var/tmp/OME/WorkerPIDs (or wherever the temp
directory is).  When a worker starts up, the first thing it does is acquire a
lock on this file and determine how many PIDs are in it.  If there are less
than MaxWorkers for this node, then it writes its own PID in the file and
continues.  If not, it responds with a 503 status (BUSY).

Meanwhile, back at the farm, the executor has been called a bunch of times and
finally reaches waitForAnyModules.  This method blocks until at least one
worker is finished.  The method returns immediately if there are no busy workers
(registered in the DB) for this executor instance.  Since workers signal they are
finished by sending IPC notices, this method is nothing more than a listener for
these notices (NotificationManager->listen()).  The listen() method is called
with a timeout so that the executor doesn't ever get stuck if messages aren't
issued (or received) for whatever reason.  If the timeout expires, then the
executor counts how many busy workers there are for its instance, and if that
number is less than the initial count when waitForAnyModules was called, it
returns. If a notice includes the executor's unique ID (MasterID), the executor
knows that one of its own jobs is finished.  A generic notice means that a worker
working with some other executor (but on the same DB) became idle.  Either way,
a call is placed to shiftQueue before returning.  The executor registers its
listeners in its constructor and unregisters them in the destructor.

